#!/usr/bin/env python3
"""
üß™ Test Script for Fine-tuned Multilingual Model

This script loads the fine-tuned model and tests generation capabilities
across all Indian languages the model was trained on:
- Hindi (hi), Tamil (ta), Telugu (te), Malayalam (ml), Kannada (kn)
- Gujarati (gu), Marathi (mr), Bengali (bn), Assamese (as), Odia (or), Punjabi (pa)

USAGE:
    python test_finetuned_model.py [--model_path MODEL_PATH] [--base_model BASE_MODEL]

EXAMPLES:
    # Use default paths
    python test_finetuned_model.py
    
    # Specify custom model path
    python test_finetuned_model.py --model_path ./outputs/lora_model
    
    # Specify both model path and base model
    python test_finetuned_model.py --model_path ./lora_model --base_model unsloth/Llama-3.2-3B-Instruct-bnb-4bit
"""

import torch
import argparse
from unsloth import FastLanguageModel
from unsloth import is_bfloat16_supported

# ============================================================================
# Parse Command Line Arguments
# ============================================================================
parser = argparse.ArgumentParser(description="Test fine-tuned multilingual model")
parser.add_argument(
    "--model_path",
    type=str,
    default="lora_model",
    help="Path to the saved fine-tuned model (default: 'lora_model')"
)
parser.add_argument(
    "--base_model",
    type=str,
    default="unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
    help="Base model name used for training (default: 'unsloth/Llama-3.2-3B-Instruct-bnb-4bit')"
)
parser.add_argument(
    "--max_new_tokens",
    type=int,
    default=256,
    help="Maximum number of tokens to generate (default: 256)"
)
parser.add_argument(
    "--temperature",
    type=float,
    default=0.7,
    help="Temperature for generation (default: 0.7)"
)
parser.add_argument(
    "--top_p",
    type=float,
    default=0.9,
    help="Top-p (nucleus) sampling parameter (default: 0.9)"
)
parser.add_argument(
    "--repetition_penalty",
    type=float,
    default=1.2,
    help="Repetition penalty to prevent repetitive outputs (default: 1.2)"
)
parser.add_argument(
    "--min_length",
    type=int,
    default=10,
    help="Minimum length of generated text (default: 10)"
)

# Use parse_known_args() to ignore unknown arguments (useful in Colab/Jupyter)
# This prevents errors when Colab passes kernel-related arguments
args, unknown = parser.parse_known_args()

# Warn about unknown arguments if any (but don't fail)
if unknown:
    print(f"Note: Ignoring unknown arguments: {unknown}")

# ============================================================================
# Model Loading Configuration
# ============================================================================
MODEL_PATH = args.model_path  # Path to the saved fine-tuned model
BASE_MODEL = args.base_model  # Base model used for training
max_seq_length = 2048
dtype = None  # None for auto detection
load_in_4bit = True  # Use 4bit quantization

# ============================================================================
# Load Fine-tuned Model
# ============================================================================
print("="*60)
print("Loading Fine-tuned Model")
print("="*60)

import os

# Check if model path exists
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(
        f"Model path '{MODEL_PATH}' not found. "
        f"Please ensure you have run the training script and the model is saved."
    )

model_loaded = False

# Method 1: Try loading as a LoRA adapter (most common case)
try:
    print(f"Method 1: Loading base model and LoRA adapter...")
    print(f"  Base model: {BASE_MODEL}")
    
    # Load base model
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=BASE_MODEL,
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,
    )
    
    # Load LoRA adapter using PEFT
    from peft import PeftModel
    print(f"  Loading LoRA adapter from: {MODEL_PATH}")
    model = PeftModel.from_pretrained(model, MODEL_PATH)
    
    # Enable inference mode
    FastLanguageModel.for_inference(model)
    
    print("‚úì Model loaded successfully (LoRA adapter)!")
    model_loaded = True
    print("="*60)
    
except Exception as e:
    print(f"  ‚ùå Method 1 failed: {str(e)[:200]}")
    print("\nTrying Method 2...")

# Method 2: Try loading directly (if it's a merged model or full model)
if not model_loaded:
    try:
        print(f"Method 2: Loading model directly from: {MODEL_PATH}")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=MODEL_PATH,
            max_seq_length=max_seq_length,
            dtype=dtype,
            load_in_4bit=load_in_4bit,
        )
        FastLanguageModel.for_inference(model)
        print("‚úì Model loaded successfully (direct load)!")
        model_loaded = True
        print("="*60)
    except Exception as e:
        print(f"  ‚ùå Method 2 failed: {str(e)[:200]}")

# Method 3: Try loading base model and then applying adapter manually
if not model_loaded:
    try:
        print(f"Method 3: Loading base model and applying adapter manually...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=BASE_MODEL,
            max_seq_length=max_seq_length,
            dtype=dtype,
            load_in_4bit=load_in_4bit,
        )
        
        # Try to load adapter weights manually
        from peft import PeftConfig
        config = PeftConfig.from_pretrained(MODEL_PATH)
        from peft import get_peft_model, LoraConfig
        
        # Recreate PEFT config
        peft_config = LoraConfig(
            r=config.r,
            lora_alpha=config.lora_alpha,
            target_modules=config.target_modules,
            lora_dropout=config.lora_dropout,
            bias=config.bias,
            task_type=config.task_type,
        )
        
        model = get_peft_model(model, peft_config)
        model.load_adapter(MODEL_PATH)
        FastLanguageModel.for_inference(model)
        
        print("‚úì Model loaded successfully (manual adapter load)!")
        model_loaded = True
        print("="*60)
    except Exception as e:
        print(f"  ‚ùå Method 3 failed: {str(e)[:200]}")

if not model_loaded:
    raise RuntimeError(
        f"Failed to load model from '{MODEL_PATH}'. "
        f"Please check:\n"
        f"1. The model was saved correctly after training\n"
        f"2. The MODEL_PATH is correct\n"
        f"3. You have the required dependencies installed"
    )

# ============================================================================
# Multilingual Prompt Template (Same as Training)
# ============================================================================
multilingual_prompt = """You are a helpful multilingual assistant capable of understanding and responding in multiple Indian languages including Hindi, Tamil, Telugu, Kannada, Malayalam, Bengali, Gujarati, Marathi, Punjabi, Odia, Assamese, Urdu, and more.

Please respond to the following in the same language as the input:

{}
"""

# ============================================================================
# Test Cases for Each Language
# ============================================================================
test_cases = {
    "Hindi (hi)": [
        {
            "task": "Title Generation",
            "input": "‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§µ‡§ø‡§µ‡§ø‡§ß‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§¶‡•á‡§∂ ‡§π‡•à ‡§ú‡§π‡§æ‡§Å ‡§ï‡§à ‡§≠‡§æ‡§∑‡§æ‡§è‡§Å ‡§¨‡•ã‡§≤‡•Ä ‡§ú‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§µ‡§ø‡§≠‡§ø‡§®‡•ç‡§® ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§Ø‡§æ‡§Å ‡§î‡§∞ ‡§™‡§∞‡§Ç‡§™‡§∞‡§æ‡§è‡§Å ‡§π‡•à‡§Ç‡•§",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?",
            "expected_type": "general"
        }
    ],
    "Tamil (ta)": [
        {
            "task": "Title Generation",
            "input": "‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç‡Æ®‡Ææ‡Æü‡ØÅ ‡Æ§‡ØÜ‡Æ©‡Øç‡Æ©‡Æø‡Æ®‡Øç‡Æ§‡Æø‡ÆØ‡Ææ‡Æµ‡Æø‡Æ≤‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥ ‡Æí‡Æ∞‡ØÅ ‡ÆÆ‡Ææ‡Æ®‡Æø‡Æ≤‡ÆÆ‡Øç. ‡Æá‡Æ§‡ØÅ ‡Æ™‡Æ¥‡ÆÆ‡Øà‡ÆØ‡Ææ‡Æ© ‡ÆÆ‡Øä‡Æ¥‡Æø ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æï‡Æ≤‡Ææ‡Æö‡Øç‡Æö‡Ææ‡Æ∞‡Æ§‡Øç‡Æ§‡Øà‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡Æö‡ØÜ‡Æ©‡Øç‡Æ©‡Øà‡ÆØ‡Æø‡Æ©‡Øç ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ ‡Æï‡Æü‡Æ±‡Øç‡Æï‡Æ∞‡Øà ‡Æé‡Æ§‡ØÅ?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡Æï‡Æ£‡Æø‡Æ©‡Æø ‡Æï‡Æ±‡Øç‡Æ±‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ±‡Ææ‡Æ≤‡Øç ‡Æé‡Æ©‡Øç‡Æ©?",
            "expected_type": "general"
        }
    ],
    "Telugu (te)": [
        {
            "task": "Title Generation",
            "input": "‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≠‡∞æ‡∞∞‡∞§‡∞¶‡±á‡∞∂‡∞Ç‡∞≤‡±ã‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡∞ß‡∞æ‡∞® ‡∞≠‡∞æ‡∞∑‡∞≤‡∞≤‡±ã ‡∞í‡∞ï‡∞ü‡∞ø. ‡∞á‡∞¶‡∞ø ‡∞Ü‡∞Ç‡∞ß‡±ç‡∞∞‡∞™‡±ç‡∞∞‡∞¶‡±á‡∞∂‡±ç ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞§‡±Ü‡∞≤‡∞Ç‡∞ó‡∞æ‡∞£ ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞æ‡∞≤‡∞≤‡±ã ‡∞Æ‡∞æ‡∞ü‡±ç‡∞≤‡∞æ‡∞°‡∞¨‡∞°‡±Å‡∞§‡±Å‡∞Ç‡∞¶‡∞ø.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡∞π‡±à‡∞¶‡∞∞‡∞æ‡∞¨‡∞æ‡∞¶‡±ç ‡∞è ‡∞∞‡∞æ‡∞∑‡±ç‡∞ü‡±ç‡∞∞‡∞Ç‡∞≤‡±ã ‡∞â‡∞Ç‡∞¶‡∞ø?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡∞ï‡±É‡∞§‡±ç‡∞∞‡∞ø‡∞Æ ‡∞Æ‡±á‡∞ß‡∞∏‡±ç‡∞∏‡±Å ‡∞Ö‡∞Ç‡∞ü‡±á ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?",
            "expected_type": "general"
        }
    ],
    "Malayalam (ml)": [
        {
            "task": "Title Generation",
            "input": "‡¥ï‡µá‡¥∞‡¥≥‡¥Ç ‡¥á‡¥®‡µç‡¥§‡µç‡¥Ø‡¥Ø‡µÅ‡¥ü‡µÜ ‡¥§‡µÜ‡¥ï‡µç‡¥ï‡µÅ‡¥™‡¥ü‡¥ø‡¥û‡µç‡¥û‡¥æ‡¥±‡µª ‡¥∏‡¥Ç‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Æ‡¥æ‡¥£‡µç. ‡¥á‡¥§‡µç ‡¥∏‡¥Æ‡µÉ‡¥¶‡µç‡¥ß‡¥Æ‡¥æ‡¥Ø ‡¥∏‡¥Ç‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥µ‡µÅ‡¥Ç ‡¥∏‡¥æ‡¥π‡¥ø‡¥§‡µç‡¥Ø‡¥µ‡µÅ‡¥Ç ‡¥â‡¥≥‡µç‡¥≥‡¥§‡¥æ‡¥£‡µç.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡¥ï‡µá‡¥∞‡¥≥‡¥§‡µç‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥§‡¥≤‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥Ç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡¥°‡¥æ‡¥±‡µç‡¥±‡¥æ ‡¥∏‡¥Ø‡µª‡¥∏‡µç ‡¥é‡¥®‡µç‡¥§‡¥æ‡¥£‡µç?",
            "expected_type": "general"
        }
    ],
    "Kannada (kn)": [
        {
            "task": "Title Generation",
            "input": "‡≤ï‡≤∞‡≥ç‡≤®‡≤æ‡≤ü‡≤ï ‡≤¶‡≤ï‡≥ç‡≤∑‡≤ø‡≤£ ‡≤≠‡≤æ‡≤∞‡≤§‡≤¶‡≤≤‡≥ç‡≤≤‡≤ø‡≤∞‡≥Å‡≤µ ‡≤í‡≤Ç‡≤¶‡≥Å ‡≤∞‡≤æ‡≤ú‡≥ç‡≤Ø. ‡≤á‡≤¶‡≥Å ‡≤∏‡≤Ç‡≤™‡≤®‡≥ç‡≤Æ‡≥Ç‡≤≤‡≤ó‡≤≥‡≤ø‡≤Ç‡≤¶ ‡≤∏‡≤Æ‡≥É‡≤¶‡≥ç‡≤ß‡≤µ‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≤ú‡≥ç‡≤û‡≤æ‡≤® ‡≤ï‡≥á‡≤Ç‡≤¶‡≥ç‡≤∞‡≤µ‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡≤¨‡≥Ü‡≤Ç‡≤ó‡≤≥‡≥Ç‡≤∞‡≤ø‡≤® ‡≤Æ‡≥Å‡≤ñ‡≥ç‡≤Ø ‡≤â‡≤¶‡≥ç‡≤Ø‡≤æ‡≤®‡≤µ‡≤® ‡≤Ø‡≤æ‡≤µ‡≥Å‡≤¶‡≥Å?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡≤®‡≥ç‡≤Ø‡≥Ç‡≤∞‡≤≤‡≥ç ‡≤®‡≥Ü‡≤ü‡≥ç‡≤µ‡≤∞‡≥ç‡≤ï‡≥ç ‡≤é‡≤Ç‡≤¶‡≤∞‡≥á‡≤®‡≥Å?",
            "expected_type": "general"
        }
    ],
    "Gujarati (gu)": [
        {
            "task": "Title Generation",
            "input": "‡™ó‡´Å‡™ú‡™∞‡™æ‡™§ ‡™™‡™∂‡´ç‡™ö‡™ø‡™Æ ‡™≠‡™æ‡™∞‡™§‡™Æ‡™æ‡™Ç ‡™Ü‡™µ‡´á‡™≤‡´Å‡™Ç ‡™è‡™ï ‡™∞‡™æ‡™ú‡´ç‡™Ø ‡™õ‡´á. ‡™§‡´á ‡™â‡™¶‡´ç‡™Ø‡´ã‡™ó ‡™Ö‡™®‡´á ‡™µ‡´á‡™™‡™æ‡™∞ ‡™Æ‡™æ‡™ü‡´á ‡™™‡´ç‡™∞‡™ñ‡´ç‡™Ø‡™æ‡™§ ‡™õ‡´á.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡™Ö‡™Æ‡™¶‡™æ‡™µ‡™æ‡™¶ ‡™ï‡™Ø‡™æ ‡™∞‡™æ‡™ú‡´ç‡™Ø‡™Æ‡™æ‡™Ç ‡™Ü‡™µ‡´á‡™≤‡´Å‡™Ç ‡™õ‡´á?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡™°‡´Ä‡™™ ‡™≤‡™∞‡´ç‡™®‡™ø‡™Ç‡™ó ‡™∂‡´Å‡™Ç ‡™õ‡´á?",
            "expected_type": "general"
        }
    ],
    "Marathi (mr)": [
        {
            "task": "Title Generation",
            "input": "‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ ‡§≠‡§æ‡§∞‡§§‡§æ‡§§‡•Ä‡§≤ ‡§è‡§ï ‡§∞‡§æ‡§ú‡•ç‡§Ø ‡§Ü‡§π‡•á. ‡§Ø‡•á‡§•‡•á ‡§∏‡§Æ‡•É‡§¶‡•ç‡§ß ‡§á‡§§‡§ø‡§π‡§æ‡§∏ ‡§Ü‡§£‡§ø ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡•Ä ‡§Ü‡§π‡•á.",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡§Æ‡•Å‡§Ç‡§¨‡§à‡§ö‡•Ä ‡§≤‡•ã‡§ï‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§ï‡§ø‡§§‡•Ä ‡§Ü‡§π‡•á?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡§®‡•á‡§ö‡§∞‡§≤ ‡§≤‡§Å‡§ó‡•ç‡§µ‡•á‡§ú ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏‡§ø‡§Ç‡§ó ‡§Æ‡•ç‡§π‡§£‡§ú‡•á ‡§ï‡§æ‡§Ø?",
            "expected_type": "general"
        }
    ],
    "Bengali (bn)": [
        {
            "task": "Title Generation",
            "input": "‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨ ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø‡•§ ‡¶è‡¶ü‡¶ø ‡¶∏‡¶æ‡¶π‡¶ø‡¶§‡ßç‡¶Ø ‡¶ì ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶ñ‡ßç‡¶Ø‡¶æ‡¶§‡•§",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡¶ï‡¶≤‡¶ï‡¶æ‡¶§‡¶æ ‡¶ï‡ßã‡¶® ‡¶∞‡¶æ‡¶ú‡ßç‡¶Ø‡ßá‡¶∞ ‡¶∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡¶Æ‡ßá‡¶∂‡¶ø‡¶® ‡¶≤‡¶æ‡¶∞‡ßç‡¶®‡¶ø‡¶Ç ‡¶ï‡¶ø?",
            "expected_type": "general"
        }
    ],
    "Assamese (as)": [
        {
            "task": "Title Generation",
            "input": "‡¶Ü‡¶∏‡¶æ‡¶Æ ‡¶â‡¶§‡ßç‡¶§‡ß∞-‡¶™‡ßÇ‡¶¨ ‡¶≠‡¶æ‡ß∞‡¶§‡ß∞ ‡¶è‡¶ñ‡¶® ‡ß∞‡¶æ‡¶ú‡ßç‡¶Ø‡•§ ‡¶á‡¶Ø‡¶º‡¶æ‡ß∞ ‡¶™‡ßç‡ß∞‡¶æ‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï ‡¶∏‡¶Æ‡ßç‡¶™‡¶¶ ‡¶Ü‡ß∞‡ßÅ ‡¶∏‡¶æ‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø‡¶ï ‡¶ê‡¶§‡¶ø‡¶π‡ßç‡¶Ø ‡¶Ü‡¶õ‡ßá‡•§",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡¶ó‡ßÅ‡ß±‡¶æ‡¶π‡¶æ‡¶ü‡ßÄ ‡¶ï‡ßã‡¶® ‡ß∞‡¶æ‡¶ú‡ßç‡¶Ø‡ß∞ ‡ß∞‡¶æ‡¶ú‡¶ß‡¶æ‡¶®‡ßÄ?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡¶ï‡ßÉ‡¶§‡ßç‡ß∞‡¶ø‡¶Æ ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶§‡ßç‡¶§‡¶æ ‡¶ï‡¶ø?",
            "expected_type": "general"
        }
    ],
    "Odia (or)": [
        {
            "task": "Title Generation",
            "input": "‡¨ì‡¨°‡¨º‡¨ø‡¨∂‡¨æ ‡¨™‡≠Ç‡¨∞‡≠ç‡¨¨ ‡¨≠‡¨æ‡¨∞‡¨§‡¨∞‡≠á ‡¨Ö‡¨¨‡¨∏‡≠ç‡¨•‡¨ø‡¨§ ‡¨è‡¨ï ‡¨∞‡¨æ‡¨ú‡≠ç‡≠ü‡•§ ‡¨è‡¨π‡¨æ‡¨∞ ‡¨∏‡¨Æ‡≠É‡¨¶‡≠ç‡¨ß ‡¨∏‡¨Ç‡¨∏‡≠ç‡¨ï‡≠É‡¨§‡¨ø ‡¨è‡¨¨‡¨Ç ‡¨á‡¨§‡¨ø‡¨π‡¨æ‡¨∏ ‡¨Ö‡¨õ‡¨ø‡•§",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡¨≠‡≠Å‡¨¨‡¨®‡≠á‡¨∂‡≠ç‡≠±‡¨∞ ‡¨ï‡≠á‡¨â‡¨Å ‡¨∞‡¨æ‡¨ú‡≠ç‡≠ü‡¨∞ ‡¨∞‡¨æ‡¨ú‡¨ß‡¨æ‡¨®‡≠Ä?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡¨ï‡¨Æ‡≠ç‡¨™‡≠ç‡≠ü‡≠Å‡¨ü‡¨∞ ‡¨¨‡¨ø‡¨ú‡≠ç‡¨û‡¨æ‡¨® ‡¨ï‡¨£?",
            "expected_type": "general"
        }
    ],
    "Punjabi (pa)": [
        {
            "task": "Title Generation",
            "input": "‡®™‡©∞‡®ú‡®æ‡®¨ ‡®â‡©±‡®§‡®∞‡©Ä ‡®≠‡®æ‡®∞‡®§ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®•‡®ø‡®§ ‡®á‡©±‡®ï ‡®∞‡®æ‡®ú ‡®π‡©à‡•§ ‡®á‡®∏ ‡®¶‡©Ä ‡®ñ‡©á‡®§‡©Ä‡®¨‡®æ‡©ú‡©Ä ‡®Ö‡®§‡©á ‡®∏‡©±‡®≠‡®ø‡®Ü‡®ö‡®æ‡®∞‡®ï ‡®µ‡®ø‡®∞‡®æ‡®∏‡®§ ‡®Æ‡®∏‡®º‡®π‡©Ç‡®∞ ‡®π‡©à‡•§",
            "expected_type": "title"
        },
        {
            "task": "Question Answering",
            "input": "‡®Ö‡©∞‡®Æ‡©ç‡®∞‡®ø‡®§‡®∏‡®∞ ‡®ï‡®ø‡®π‡©ú‡©á ‡®∞‡®æ‡®ú ‡®µ‡®ø‡©±‡®ö ‡®π‡©à?",
            "expected_type": "qa"
        },
        {
            "task": "General Conversation",
            "input": "‡®°‡®æ‡®ü‡®æ ‡®µ‡®ø‡®∏‡®º‡®≤‡©á‡®∏‡®º‡®£ ‡®ï‡©Ä ‡®π‡©à?",
            "expected_type": "general"
        }
    ]
}

# ============================================================================
# Generation Function
# ============================================================================
def generate_response(prompt, max_new_tokens=None, temperature=None, top_p=None, task_type="general"):
    """Generate response from the model with improved parameters."""
    # Use command-line arguments if not provided
    if max_new_tokens is None:
        max_new_tokens = args.max_new_tokens
    if temperature is None:
        temperature = args.temperature
    if top_p is None:
        top_p = args.top_p
    
    # Adjust parameters based on task type
    if task_type == "title":
        # For titles, use lower temperature and shorter max tokens
        temperature = min(temperature, 0.5)
        max_new_tokens = min(max_new_tokens, 50)
    elif task_type == "qa":
        # For Q&A, use moderate temperature
        temperature = min(temperature, 0.6)
    
    # Format the prompt using the same template as training
    formatted_prompt = multilingual_prompt.format(prompt)
    
    # Tokenize
    inputs = tokenizer(
        formatted_prompt,
        return_tensors="pt",
        truncation=True,
        max_length=max_seq_length
    ).to(model.device)
    
    # Generate with improved parameters
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            min_length=args.min_length,
            temperature=temperature,
            top_p=top_p,
            repetition_penalty=args.repetition_penalty,  # Prevent repetition
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3,  # Prevent 3-gram repetition
        )
    
    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract only the generated part (remove the input prompt)
    if formatted_prompt in generated_text:
        response = generated_text.split(formatted_prompt)[-1].strip()
    else:
        response = generated_text.strip()
    
    # Clean up response - remove any remaining prompt artifacts
    if response.startswith("Please respond to the following"):
        # Try to extract the actual response
        parts = response.split("\n\n")
        if len(parts) > 1:
            response = "\n\n".join(parts[1:]).strip()
    
    return response

# ============================================================================
# Run Tests
# ============================================================================
print("\n" + "="*60)
print("Running Generation Tests")
print("="*60)

results = {}

for language, test_list in test_cases.items():
    print(f"\n{'='*60}")
    print(f"Testing: {language}")
    print(f"{'='*60}")
    
    language_results = []
    
    for i, test_case in enumerate(test_list, 1):
        print(f"\n[Test {i}/{len(test_list)}] {test_case['task']}")
        print(f"Input: {test_case['input'][:100]}..." if len(test_case['input']) > 100 else f"Input: {test_case['input']}")
        print("-" * 60)
        
        try:
            # For title generation tasks, format similar to training with explicit instruction
            if test_case['expected_type'] == 'title':
                prompt = f"Given the following text section, provide an appropriate title (1-5 words only):\n\n{test_case['input']}"
            elif test_case['expected_type'] == 'qa':
                # For Q&A, make it explicit that we want a direct answer
                prompt = f"Answer the following question directly and concisely:\n\n{test_case['input']}"
            else:
                prompt = test_case['input']
            
            response = generate_response(
                prompt, 
                max_new_tokens=256,
                task_type=test_case['expected_type']
            )
            
            print(f"Generated Response:")
            print(f"{response}")
            print("-" * 60)
            
            language_results.append({
                "task": test_case['task'],
                "input": test_case['input'],
                "output": response,
                "status": "success"
            })
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            language_results.append({
                "task": test_case['task'],
                "input": test_case['input'],
                "output": None,
                "status": f"error: {str(e)}"
            })
    
    results[language] = language_results

# ============================================================================
# Summary Report
# ============================================================================
print("\n" + "="*60)
print("Test Summary Report")
print("="*60)

total_tests = 0
successful_tests = 0
failed_tests = 0

for language, language_results in results.items():
    lang_success = sum(1 for r in language_results if r['status'] == 'success')
    lang_total = len(language_results)
    total_tests += lang_total
    successful_tests += lang_success
    failed_tests += (lang_total - lang_success)
    
    status_icon = "‚úì" if lang_success == lang_total else "‚ö†"
    print(f"\n{status_icon} {language}: {lang_success}/{lang_total} tests passed")

print(f"\n{'='*60}")
print(f"Overall: {successful_tests}/{total_tests} tests passed ({100*successful_tests/total_tests:.1f}%)")
print(f"{'='*60}")

# ============================================================================
# Save Results
# ============================================================================
import json
from datetime import datetime

output_file = f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"\n‚úì Test results saved to: {output_file}")

print("\n" + "="*60)
print("Testing Complete!")
print("="*60)

